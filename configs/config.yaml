model:
  name: Qwen/Qwen2.5-Coder-3B-Instruct  # Qwen/Qwen2.5-Coder-3B-Instruct
  # Optional: pass through to HF
  tokenizer_kwargs: {}
  model_kwargs:
    trust_remote_code: true
    device_map: auto

data:
  dataset_name: FudanSELab/ClassEval
  split_ratio: 0.8  # train/eval split ratio

collab:
  mode: TAKE_JOB           # ONE | TAKE_JOB
  num_agents: 3       # used when mode=TAKE_JOB

external:
  mode: code_feedback   # plain | plain_simple | code_feedback
  original_prompt: true  # include skeleton and instructions each turn
  previous_response: true  # include previous implementations each turn

trainer:
  output_dir: TODO  # trainer output/checkpoint directory (set via scripts/train_grpo_sbatch_ghx4.sh)
  num_train_epochs: 3
  per_device_train_batch_size: 1
  # Learning rate for optimizer (alias: lr)
  learning_rate: 1.7e-5
  logging_steps: 50
  save_steps: 200
  num_generations: 3
  # Per-agent generation cap; increase if outputs truncate.
  max_new_tokens: 660
  temperature: 0.25
  top_p: 0.90
  num_turns: 2
  # PPO-related (CoMLRL MAGRPO) options
  # Whether to normalize advantages when updating policy
  # normalize_advantage: true
  # Clipping coefficient (a.k.a. epsilon clip). Set null to use library default.
  # epsilon_clip: 12

reward_processor:
  enabled: false
  scale_factor: 1.0
  shift: null

wandb:
  enabled: true
  project: classeval
  entity: null
  output_dir: TODO  # directory for local W&B files (set via scripts/train_grpo_sbatch_ghx4.sh)

output:
  save_final_model: true
  save_path: TODO  # final merged policy/model output path (set via scripts/train_grpo_sbatch_ghx4.sh)
  keep_tmp: true
  tmp_base_dir: TODO  # scratch dir for code execution artifacts (set via scripts/train_grpo_sbatch_ghx4.sh)

fixed_seed: false   # when false, seed is randomized at runtime
seed: 42           # used only when fixed_seed=true
