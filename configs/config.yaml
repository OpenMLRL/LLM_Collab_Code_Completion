model:
  name: Qwen/Qwen2.5-Coder-3B-Instruct  # Qwen/Qwen2.5-Coder-3B-Instruct
  # Optional: pass through to HF
  tokenizer_kwargs: {}
  model_kwargs:
    trust_remote_code: true
    device_map: auto

data:
  dataset_name: FudanSELab/ClassEval
  split_ratio: 0.8  # train/eval split ratio

collab:
  mode: TAKE_JOB           # ONE | RAND_PARTITION | TAKE_JOB
  num_agents: 3       # used when mode=RAND_PARTITION or TAKE_JOB

external:
  mode: personal_feedback   # plain | passed | level_feedback | level_passed | group_feedback | personal_feedback
  original_prompt: true  # include skeleton and instructions each turn
  previous_response: true  # include previous implementations each turn

trainer:
  output_dir: /projects/bevi/tchen19/output/CE/[jobid]
  num_train_epochs: 5
  per_device_train_batch_size: 1
  # Learning rate for optimizer (alias: lr)
  learning_rate: 1.7e-5
  logging_steps: 50
  save_steps: 200
  num_generations: 3
  # Per-agent generation cap; increase if outputs truncate.
  max_new_tokens: 660
  temperature: 0.25
  top_p: 0.90
  num_turns: 2
  # PPO-related (CoMLRL MAGRPO) options
  # Whether to normalize advantages when updating policy
  # normalize_advantage: true
  # Clipping coefficient (a.k.a. epsilon clip). Set null to use library default.
  # epsilon_clip: 12

reward_processor:
  enabled: false
  scale_factor: 1.0
  shift: null

wandb:
  enabled: true
  project: classeval
  entity: null
  output_dir: /projects/bevi/tchen19/output/CE/[jobid]/wandb

output:
  save_final_model: true
  save_path: /projects/bevi/tchen19/output/CE/[jobid]/output
  keep_tmp: true
  tmp_base_dir: /projects/bevi/tchen19/output/CE/[jobid]/tmp

fixed_seed: false   # when false, seed is randomized at runtime
seed: 42           # used only when fixed_seed=true
